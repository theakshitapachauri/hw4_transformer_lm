{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text with your tranied lagnuage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformer_lm.modeling_transformer import TransformerLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see error `404 Client Error: Not Found` it probably means that \"../output\" is not the correct directory with your tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Repository Not Found for url: https://huggingface.co/output_dir/resolve/main/tokenizer_config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "../output_dir is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mget_file_from_repo\u001B[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001B[0m\n\u001B[1;32m   2234\u001B[0m         \u001B[0;31m# Load from URL or cache if already cached\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2235\u001B[0;31m         resolved_file = cached_path(\n\u001B[0m\u001B[1;32m   2236\u001B[0m             \u001B[0mresolved_file\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   1845\u001B[0m         \u001B[0;31m# URL, so get it from the cache (downloading if necessary)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1846\u001B[0;31m         output_path = get_from_cache(\n\u001B[0m\u001B[1;32m   1847\u001B[0m             \u001B[0murl_or_filename\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   2049\u001B[0m             \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mallow_redirects\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0metag_timeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2050\u001B[0;31m             \u001B[0m_raise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2051\u001B[0m             \u001B[0metag\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"X-Linked-Etag\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ETag\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36m_raise_for_status\u001B[0;34m(request)\u001B[0m\n\u001B[1;32m   1970\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0merror_code\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"RepoNotFound\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1971\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mRepositoryNotFoundError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"404 Client Error: Repository Not Found for url: {request.url}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1972\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0merror_code\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"EntryNotFound\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m: 404 Client Error: Repository Not Found for url: https://huggingface.co/output_dir/resolve/main/tokenizer_config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/k7/_59mx83x42b_7nm36p7j5ktc0000gn/T/ipykernel_12111/4059864520.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPreTrainedTokenizerFast\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"../output_dir\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTransformerLM\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"../output_dir\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m;\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[1;32m   1653\u001B[0m             \u001B[0;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1654\u001B[0m             \u001B[0mfast_tokenizer_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mFULL_TOKENIZER_FILE\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1655\u001B[0;31m             resolved_config_file = get_file_from_repo(\n\u001B[0m\u001B[1;32m   1656\u001B[0m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1657\u001B[0m                 \u001B[0mTOKENIZER_CONFIG_FILE\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/file_utils.py\u001B[0m in \u001B[0;36mget_file_from_repo\u001B[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001B[0m\n\u001B[1;32m   2245\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mRepositoryNotFoundError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2246\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2247\u001B[0;31m         raise EnvironmentError(\n\u001B[0m\u001B[1;32m   2248\u001B[0m             \u001B[0;34mf\"{path_or_repo} is not a local folder and is not a valid model identifier \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2249\u001B[0m             \u001B[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOSError\u001B[0m: ../output_dir is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"../output_dir\")\n",
    "model = TransformerLM.from_pretrained(\"../output_dir\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use method `.generate()` to continue your text. This is how it works:\n",
    "\n",
    "1. It accepts a batch of input_ids (batch can contain consist of a single example, but should have the shape [1, sequence_length]).\n",
    "1. The model generates a probability distribution over your vocabulary and selects the token_id with the highest probability.\n",
    "1. This token_id is added to the input_ids and the process repeats untill you reach sequence size = max_length\n",
    "\n",
    "This is a very crude and simple way of generating text. You may notice that the model tends to repeat the same word after a certain point (however, if produces the same word all the time, it is probably not trained perfectly). We will learn more ways of how to generate text given the distribution of the next token later in class.\n",
    "\n",
    "You can find the implementation of `.generate()` in `transformer_lm/modeling_transformer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please keep this example as it is.\n",
    "input_ids = tokenizer(\"The meaning of life is\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_length=30)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to generate more text.\n",
    "### Play with different prefixes and max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"You can put any text here\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "generated_ids = model.generate(input_ids, max_length=30)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "generated_text"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f86d5ac646110a331c60478f9400a1a64d0cd523be90d887457228f9723636b5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('nlp_class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}